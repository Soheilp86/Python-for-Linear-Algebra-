{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chapter serves as introductory exploration of fundamental concepts in linear algebra with a focus on computational aspects. It also serves as foundational background for the subsequent chapter, \"Linear Algebra and Optimization for Data Analysis\".\n",
    "\n",
    "We will begin by exploring systems of linear equations, also known as linear systems, which play a central role in linear algebra. In fact, many problems in linear algebra can be converted in to solving a linear system.\n",
    "\n",
    "Linear systems and their solutions can be described using matrices. We will study the algebraic properties of matrices in Section 2. These properties can be used to solve a linear system and set a groundwork for topics covered in following sections.\n",
    "\n",
    "In Section 3, we will discuss linear transformation between subspaces of $\\mathbb{R}^n$, and explore how changes of bases affects the representation of these transformations. We won't delve into vector spaces in their general form. This approach maintains generality as an $n$-dimensional vector space and $\\mathbb{R}^n$ are the same from a computational point of view.\n",
    "\n",
    "Section 4 is devoted to dissecting the action of linear maps into elements that are easiy to visualize, using eigenvalues and eigenvectors. We will delve into how a basis formed by eigenvectors leads to diagonal matrices (diagonalization). we will also examine the application of diagonalization in dealing with large matrix products and discrete dynamical systems.\n",
    "\n",
    "Section 5 explores some ideas from analytic geometry, which allows us to define intuitive geometric concepts such as length, distance, and perpendicularity in $\\mathbb{R}^n$. It will also dicuss orthogonal projection, Gram-Schmidt algorithm, and the QR- factorization which are key concepts in many calculations involving orthogonality. Finally, we will discuss the least square problem, an important concept in regression analysis, to approximate the solution of linear systems in which there are more equations than unknowns.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Each section begins with a brief overview of the objectives and followed by several examples exercises  to enhance understanding and facilitate learning. Moreover, we strive to include numerical notes wherever appropriate. These notes aim to explain and compare different computational approaches to problem-solving.\n",
    "Our goal is to write our own code (whenever possible) to solve problems. However, for matrices of higher dimensions, we take advantage of the [NumPy linear algebra](https://numpy.org/devdocs/reference/routines.linalg.html) module which provide efficient implementations of standard linear algebra algorithms.\n",
    "\n",
    "\n",
    "The chapter covers the following topics:\n",
    "\n",
    "1. Linear Systems.\n",
    "2. Matrix Algebra.\n",
    "3. Linear Transformations.\n",
    "4. Eigenvalues and Eigenvectors.\n",
    "5. Analytic Geometry.\n",
    "\n",
    "The content of this chapter is based on:\n",
    "\n",
    "- Lay, David, et al. Linear Algebra and its Applications. 5th ed., Pearson., 2016\n",
    "- Deisenroth, Marc Peter, A. Aldo Faisal, and Cheng Soon Ong. Mathematics for Machine Learning. Cambridge University Press, 2020.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
