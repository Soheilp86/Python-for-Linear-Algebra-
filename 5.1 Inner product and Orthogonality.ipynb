{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6351a93a",
   "metadata": {},
   "source": [
    "# 5.1 Inner product and Orthogonality\n",
    "\n",
    "In this section, we explore inner product and orthogonality. Inner product allows us to define intuitive geometric concepts such as length, distance, and perpendicularity in $n$ dimensional space $\\mathbb{R}^n$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e81d0b",
   "metadata": {},
   "source": [
    "## Inner product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faca33ec",
   "metadata": {},
   "source": [
    "Let $\\vec{u}$ and $\\vec{v}$ be vectors in $\\mathbb{R}^n$. We can view them as $n \\times 1$ matrices. The transpose of $\\vec{u}$, denoted as $\\vec{u}^T$, is a $1\\times n$ matrix. The matrix product of $\\vec{u}^T$ and $\\vec{v}$ yields a $1\\times 1$ matrix, which can be written as a real number (a scalar) without brackets. This scalar is known as the inner product or dot product of $\\vec{u}$ and $\\vec{v}$. To compute it, we multiply the corresponding elements of $\\vec{u}$ and $\\vec{v}$ and sum the results. More precisely, if $\\vec{u} = \\begin{bmatrix} u_1\\\\ u_2 \\\\ \\vdots \\\\ u_n \\end{bmatrix}$ and $\\vec{v} = \\begin{bmatrix} v_1\\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{bmatrix}$, then the dot product $\\vec{u} \\cdot \\vec{v}$ is given by:\n",
    "\n",
    "$$\n",
    "\\vec{u} \\cdot \\vec{v} = \\vec{u}^T\\vec{v} = \\begin{bmatrix} u_1 & u_2 & \\dots & u_n \\end{bmatrix} \\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{bmatrix} = u_1v_1 + u_2v_2 + \\dots + u_nv_n\n",
    "$$\n",
    "\n",
    "__Example 1:__ \n",
    "\n",
    "Compute $\\vec{u} \\cdot \\vec{v}$ for $\\vec{u} = \\begin{bmatrix} 1\\\\ -1 \\\\ 2 \\end{bmatrix}$ and $\\vec{v} = \\begin{bmatrix} -2 \\\\ 3 \\\\ 4 \\end{bmatrix}$.\n",
    "\n",
    "\n",
    "\n",
    "__Solution__ \n",
    "\n",
    "We use `numpy.dot()` to compute the dot product in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "11774a61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "u = np.array([1, -1, 2])\n",
    "v = np.array([-2, 3, 4])\n",
    "\n",
    "uv = np.dot(u,v)\n",
    "uv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85be6ec5",
   "metadata": {},
   "source": [
    "__Theorem 1__ (Properties of the dot product)\n",
    "\n",
    "Let $\\vec{u}$, $\\vec{v}$ and $\\vec{w}$ be vectors in $\\mathbb{R}^n$, and $c \\in \\mathbb{R}$ be a scalar. Then\n",
    "\n",
    "  1. $\\vec{u}\\cdot\\vec{v} = \\vec{v}\\cdot\\vec{u}.$\n",
    "  \n",
    "  2. $(\\vec{u} + \\vec{v})\\cdot \\vec{w} = \\vec{u}\\cdot\\vec{w} + \\vec{v}\\cdot\\vec{w}.$\n",
    "  \n",
    "  3. $c(\\vec{u}\\cdot\\vec{v}) = (c\\vec{u})\\cdot\\vec{v} = \\vec{u}\\cdot(c\\vec{v}).$\n",
    "  \n",
    "  4. $\\vec{u}\\cdot\\vec{v}\\geq 0$, and $\\vec{u}\\cdot\\vec{u} = 0$ if and only if $\\vec{u}=0.$\n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "combining (2) and (3), and using induction we can show:\n",
    "  \n",
    "$$(c_1\\vec{u_1} + c_2\\vec{u}_2 \\dots c_p\\vec{u}_p)\\cdot \\vec{w} = c_1\\vec{w} \\cdot \\vec{u}_1 + c_2\\vec{w} \\cdot \\vec{u}_2 + \\dots c_p\\vec{w} \\cdot \\vec{u}_p$$\n",
    "  \n",
    "  \n",
    "\n",
    "__Length of vectors__\n",
    "\n",
    "Dot product can be used to define the length of vectors: let $\\vec{u}\\in \\mathbb{R}^n$ then the magnitude or the _length_ of $\\vec{u}$, denoted by $\\|\\vec{u}\\|$ is defined by\n",
    "\n",
    "$$\n",
    "\\|\\vec{u}\\| = \\sqrt{\\vec{u} \\cdot \\vec{u}}.\n",
    "$$\n",
    " \n",
    "\n",
    "- Note that this definition of length coincide with the standard notion of length in $\\mathbb{R}^2$ and $\\mathbb{R}^3$.\n",
    "\n",
    "\n",
    "- A vector with a length of $1$ is called as a unit vector. For any vector $\\vec{u}$, there exists a unit vector in the direction of $\\vec{u}$. To obtain this vector, we first calculate the length of $\\vec{u}$ and then divide $\\vec{u}$ by its length $\\|\\vec{u}\\|$. The resulting vector is referred to as the unit vector of $\\vec{u}$, and commonly denoted as $\\vec{e}_u$:\n",
    "\n",
    "$$\n",
    "\\vec{e}_u = \\frac{\\vec{u}}{\\|\\vec{u}\\|}\n",
    "$$\n",
    "\n",
    "This process is often called normalizing a vector, as it transforms $\\vec{u}$ into a unit vector by scaling it to have a length of $1$.\n",
    "\n",
    "__Example 1__\n",
    "\n",
    "Let $\\vec{u} = \\begin{bmatrix} 1\\\\ 2 \\\\ 3 \\end{bmatrix}$. Compute the following:\n",
    "\n",
    "1. $\\|\\vec{u}\\|$\n",
    "\n",
    "2. $\\|-2\\vec{u}\\|$\n",
    "\n",
    "3. $\\vec{e}_u$\n",
    "\n",
    "__Solution:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "34f52f9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7416573867739413"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (1)\n",
    "\n",
    "u = np.array([1,2,3])\n",
    "\n",
    "uu = np.dot(u,u)\n",
    "\n",
    "length_u = np.sqrt(uu)\n",
    "\n",
    "length_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6504d859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.483314773547883"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (2)\n",
    "\n",
    "v = -2*u\n",
    "\n",
    "vv = np.dot(v,v)\n",
    "\n",
    "length_v = np.sqrt(vv)\n",
    "\n",
    "length_v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d16227",
   "metadata": {},
   "source": [
    "We could also use the properties of the dot products to compute $\\|-2\\vec{u}\\|$:\n",
    "    \n",
    "$$\\|-2\\vec{u}\\| = \\sqrt{-2\\vec{u}\\cdot -2 \\vec{u}} = 2 (\\vec{u}\\cdot\\vec{u}) = 2 \\|\\vec{u}\\|$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "03713c05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.26726124, 0.53452248, 0.80178373])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#(3) comuting the unit vector of u\n",
    "\n",
    "e_u = u/length_u\n",
    "\n",
    "e_u"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c91836b",
   "metadata": {},
   "source": [
    "Let's check that the length of $\\vec{e}_u$ is exactly 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "73f6b1b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length_e = np.sqrt(np.dot(e_u,e_u))\n",
    "\n",
    "length_e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a0efe5",
   "metadata": {},
   "source": [
    "__Example 2:__\n",
    "\n",
    "Given vectors $\\vec{u} = \\begin{bmatrix} 1\\\\ -1 \\\\ 2 \\end{bmatrix}$ and $\\vec{v} = \\begin{bmatrix} -2 \\\\ 3 \\\\ 4 \\end{bmatrix}$, and the subspace $W$ spanned by $\\vec{u}$ and $\\vec{v}$, find unit vectors $\\vec{w}_1$ and $\\vec{w}_2$ that form a basis for $W$.\n",
    "\n",
    "__Solution:__\n",
    "\n",
    "Since $\\vec{u}$ and $\\vec{v}$ are linearly independent (they are not scalar multiples of each other), we can proceed to normalize them to get unit vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c31d6667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_u =  [ 0.40824829 -0.40824829  0.81649658]\n",
      "e_v =  [-0.37139068  0.55708601  0.74278135]\n"
     ]
    }
   ],
   "source": [
    "# unit vector of u\n",
    "u = np.array([1,-1,2])\n",
    "e_u = u / np.sqrt(np.dot(u,u))\n",
    "\n",
    "\n",
    "# unit vector of v\n",
    "v = np.array([-2, 3, 4])\n",
    "e_v = v / np.sqrt(np.dot(v,v))\n",
    "\n",
    "\n",
    "print(\"e_u = \", e_u)\n",
    "print(\"e_v = \", e_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d17ef63",
   "metadata": {},
   "source": [
    "__Distance in $\\mathbb{R}^n$__\n",
    "\n",
    "For $\\vec{u}$ and $\\vec{v}$ in $\\mathbb{R}^n$, the distance between $\\vec{u}$ and $\\vec{v}$, denoted by $\\text{dist}\\ (\\vec{v}, \\vec{u})$, is the length of their difference vector $\\vec{u} - \\vec{v}$. That is,\n",
    "\n",
    "$$\n",
    "\\text{dist}\\ (\\vec{u}, \\vec{v}) = \\|\\vec{u} - \\vec{v}\\|.\n",
    "$$\n",
    "\n",
    "Note that this definition coincide with the standard definition of distances in $\\mathbb{R}^2$ and $\\mathbb{R}^3$.\n",
    "\n",
    "\n",
    "__Example 3__ Compute the distance between vectors in Example 2.\n",
    "\n",
    "\n",
    "__Solution:__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4019b566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The distance between  [ 1 -1  2]  and  [-2  3  4]  is  5.385164807134504\n"
     ]
    }
   ],
   "source": [
    "\n",
    "w = u - v\n",
    "\n",
    "dist = np.sqrt(np.dot(w,w))\n",
    "\n",
    "print(\"The distance between \", u, \" and \", v, \" is \", dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8712dd1",
   "metadata": {},
   "source": [
    "## Angles and Orthogonality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867c7d9f",
   "metadata": {},
   "source": [
    "\n",
    "__Theorem 2 (Cauchy-Schwarz inequality)__\n",
    "\n",
    "For $\\vec{u}$ and $\\vec{v}$ in $\\mathbb{R}^n$ we have \n",
    "\n",
    "$$\n",
    "|\\vec{u}\\cdot\\vec{v}|\\leq \\|\\vec{u}\\|\\|\\vec{v}\\|.\n",
    "$$\n",
    "\n",
    "\n",
    "The Cauchy-Schwarz inequality gives us a way to define the notion of angel between two $n$ dimensional vectors $\\vec{u}$ and $\\vec{v}$. This notion also coincide with our intuition in $\\mathbb{R}^2$ and $\\mathbb{R}^3$:\n",
    "\n",
    "Suppose $\\vec{u}, \\vec{v}\\in \\mathbb{R}^n$ are non-zero vectors. Note that \n",
    "\n",
    "$$\n",
    "-1 \\leq \\frac{\\vec{u}\\cdot\\vec{v}}{\\|\\vec{u}\\|\\|\\vec{v}\\|}\\leq 1\n",
    "$$\n",
    "\n",
    "Therefore, there is a unique $\\theta \\in [0,\\pi]$ such that \n",
    "\n",
    "$$\n",
    "\\cos(\\theta) = \\frac{\\vec{u}\\cdot\\vec{v}}{\\|\\vec{u}\\|\\|\\vec{v}\\|}\n",
    "$$\n",
    "\n",
    "$\\theta$ is reffered to as_the angel_ between $\\vec{u}$ and $\\vec{v}$ and is equal to \n",
    "\n",
    "$$\n",
    "\\theta = \\arccos(\\frac{\\vec{u}\\cdot\\vec{v}}{\\|\\vec{u}\\|\\|\\vec{v}\\|})\n",
    "$$\n",
    "\n",
    "__Example 4__\n",
    "\n",
    "Find the angel between vectors in Example 2.\n",
    "\n",
    "__Solution:__\n",
    "\n",
    "\n",
    "We first compute the length of $\\vec{u}$ and $\\vec{v}$ and then substitute them in above formula:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "06a7669c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The angel between u and v is  1.3413592242878416  rad\n"
     ]
    }
   ],
   "source": [
    "u_len = np.sqrt(np.dot(u,u))\n",
    "v_len = np.sqrt(np.dot(v,v))\n",
    "\n",
    "z = u_len * v_len\n",
    "\n",
    "t = np.arccos(np.dot(u,v)/z)       \n",
    "print(\"The angel between u and v is \", t ,\" rad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8b9d8e",
   "metadata": {},
   "source": [
    "Two vectors $\\vec{u}$ and $\\vec{v}$ are __orthogonal__, denoted by $\\vec{u} \\perp \\vec{v}$, if the angle $\\theta$ between them is $\\frac{\\pi}{2}$. Alternatively, this condition is satisfied if and only if $\\vec{u}\\cdot \\vec{v} = 0$. It is worth noting that the zero vector $\\vec{0}$ is orthogonal to every vector because $\\vec{0}^T \\cdot \\vec{u} = 0$ for any vector $\\vec{u}$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Observe that for any two vectors $\\vec{u}$ and $\\vec{v}$ we have\n",
    "\n",
    "$$\n",
    "\\|\\vec{u}+\\vec{v}\\|^2 = (\\vec{u} + \\vec{v}) \\cdot (\\vec{u} + \\vec{v}) = \\| \\vec{u}\\|^2 + \\| \\vec{v}\\|^2 + 2 \\vec{u}\\cdot \\vec{v}\n",
    "$$\n",
    "\n",
    "The above calculation implies the famous Pythagorean Theorem:\n",
    "\n",
    "__Theorem 3 (The Pythagorean Theorem)__\n",
    "\n",
    "Two vectors $\\vec{u}$ and $\\vec{v}$ are orthogonal if and only if \n",
    "\n",
    "$$\n",
    "\\|\\vec{u}+\\vec{v}\\|^2 =  \\| \\vec{u}\\|^2 + \\| \\vec{v}\\|^2 \n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "__The orthogonal complement of a subspace__\n",
    "\n",
    "Let $W \\subset \\mathbb{R}^n$ be a subspcae. If a vector $\\vec{u}$ is orthogonal to every vector in the subspace $W$, then $\\vec{u}$ is said to be orthogonal to $W$. The set of all vectors $\\vec{u}$ that are orthogonal to $W$ form a subspace which is commonly called __the orthogonal complement of $W$__ and is denoted by $W^{\\perp}$ (or simply $W^\\perp$):\n",
    "\n",
    "$$\n",
    "W^{\\perp} = \\{\\vec{u}\\in \\mathbb{R}^n: \\vec{u}\\cdot \\vec{w} = 0 \\text{ for all } \\vec{w} \\in W\\}\n",
    "$$\n",
    "\n",
    "__Example 7__\n",
    "\n",
    "Let $W$ be the $xy$-plane in $\\mathbb{R}^3$, and let $L$ be the $z$-axis. Our intuition says $L$ is orthogonal to $W$. In fact, if $\\vec{z}$ and $\\vec{w}$ are nonzero vectors such that $\\vec{z}\\in L$ and $\\vec{w}\\in W$, then $\\vec{z}$ is orthogonal to $\\vec{w}$. Since $\\vec{w}$ and $\\vec{z}$ were chosen arbitrary, every vector on $L$ is orthogonal to every $\\vec{w}$ in $W$. In fact, $L$ consists of all vectors that are orthogonal to the $\\vec{w}$'s in $W$, and $W$ consists of all vectors $\\vec{w}$ orthogonal to the $\\vec{z}$’s in $L$. Mathematically,\n",
    "\n",
    "$$\n",
    "L = W^{\\perp} \\quad \\text{and} \\quad W = L^{\\perp}\n",
    "$$\n",
    "\n",
    "\n",
    "__Example 8__:\n",
    "\n",
    "If $W = span\\left\\lbrace \\begin{bmatrix}\n",
    "1\\\\0\\\\0\n",
    "\\end{bmatrix} \\right\\rbrace$, find a basis for $W^{\\perp}$.\n",
    "\n",
    "__Solution__:\n",
    "\n",
    "Note that $W$ is simply the $x$-axis in $\\mathbb{R}^3$. Intuitively, $W^{\\perp}$ should be the $yz$-plane, and a basis for it is $B = \\left\\lbrace \\begin{bmatrix}\n",
    "0\\\\1\\\\0\n",
    "\\end{bmatrix}, \\begin{bmatrix}\n",
    "0\\\\0\\\\1\n",
    "\\end{bmatrix} \\right\\rbrace$. Alternatively, we can find the solutions to the linear system:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 0 &0 \n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "x \\\\y \\\\ z\n",
    "\\end{bmatrix} = \\vec{0}.\n",
    "$$\n",
    "\n",
    "We have:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "x \\\\y \\\\ z\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "0 \\\\y \\\\ z\n",
    "\\end{bmatrix} = y \\begin{bmatrix}\n",
    "0 \\\\1 \\\\ 0\n",
    "\\end{bmatrix} + z \\begin{bmatrix}\n",
    "0 \\\\0 \\\\ 1\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "\n",
    "We conclude this section with a theorem summarizing the properties of the orthogonal complement of a subspace.\n",
    "\n",
    "__Theorem 4__:\n",
    "\n",
    "Let $W$ be a subspace of $\\mathbb{R}^n$.\n",
    "\n",
    "1. The orthogonal complement $W^{\\perp}$ is also a subspace of $\\mathbb{R}^n$.\n",
    "\n",
    "2. The sum of the dimensions of $W$ and $W^{\\perp}$ is equal to the dimension of the ambient space $\\mathbb{R}^n$, i.e., $dim(W) + dim(W^{\\perp}) = n$.\n",
    "\n",
    "3. If $W$ is the column space of some matrix $A$, i.e., $W = \\text{col}(A)$, then $W^{\\perp}$ is precisely the null space of the transpose of $A$, i.e., $W^{\\perp} = \\text{null}(A^{T})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0265d3c3",
   "metadata": {},
   "source": [
    "## Excercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e51af08",
   "metadata": {},
   "source": [
    "1. Let  $ \\vec{a} = \\begin{bmatrix}1 \\\\ 2 \\\\ 3 \\\\ 4 \\end{bmatrix}$. \n",
    "\n",
    "    a. Find a vector $\\vec{w}$ that is in the opposite direction of $\\vec{a}$ and has a magnitude of 2.\n",
    "    \n",
    "    b. Find two non-parallel vectors $\\vec{u}$ and $\\vec{v}$ which are both orthogonal to $\\vec{a}$\n",
    "\n",
    "\n",
    "\n",
    "2. Find two non-parallel vectors $\\vec{u}$ and $\\vec{v}$ which are both orthogonal to $\\begin{bmatrix}\n",
    "1 \\\\ 2 \\\\ 3 \\end{bmatrix}$ and $\\begin{bmatrix} 2 \\\\ -1 \\\\ 0 \\end{bmatrix}.$\n",
    "\n",
    "\n",
    "3. Let $\\vec{v}= \\begin{bmatrix}\n",
    "1\\\\2\\\\7\n",
    "\\end{bmatrix}$ and  $\\vec{w}= \\begin{bmatrix}\n",
    "1\\\\-1\\\\1\n",
    "\\end{bmatrix}.$\n",
    "\n",
    "   a. Find $\\vec{z}= \\vec{v}- \\displaystyle\\left(\\frac{\\vec{v}\\cdot \\vec{w}}{\\vec{w}\\cdot \\vec{w}}\\right)\\vec{w}.$\n",
    "   \n",
    "   b. Check that $\\vec{z}$ is orthogonal to $\\vec{w}.$\n",
    "\n",
    "\n",
    "\n",
    "4. If $W = span\\left\\lbrace\\ \\begin{bmatrix}\n",
    "1\\\\2\\\\3 \\\\ 4\n",
    "\\end{bmatrix}\\ \\right\\rbrace$ find a basis for $W^{\\perp}$.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
